{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "12_transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aidot-kr/AISecurity/blob/master/12_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FA3GWvL2Ir1U"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAcQjBdOIr1Y"
      },
      "source": [
        "\n",
        "\n",
        "*   NN.TRANSFORMER 와 TORCHTEXT 로 seq2seq 모델링\n",
        "===============================================================\n",
        "출처) https://tutorials.pytorch.kr/\n",
        "\n",
        "\n",
        "![](https://github.com/pytorch/tutorials/blob/gh-pages/_static/img/transformer_architecture.jpg?raw=1)\n",
        "\n",
        "\n",
        "nn.TransformerEncoder 모델을 언어 모델링(language modeling) 과제에 적용한 예제임\n",
        "- 과제는 주어진 단어 (또는 단어의 시퀀스)가 다음에 이어지는 단어 시퀀스를 따를 가능성(likelihood)에 대한 확률을 계산하는 것임\n",
        "- 먼저, 토큰(token) 들의 시퀀스가 임베딩(embedding) 레이어로 전달되며, 이어서 포지셔널 인코딩(positional encoding) 레이어가 각 단어의 순서를 설명\n",
        "- nn.TransformerEncoder 는 여러 개의 nn.TransformerEncoderLayer 레이어로 구성되어 있음\n",
        "- nn.TransformerEncoder 내부의 셀프-어텐션(self-attention) 레이어들은 시퀀스 안에서의 이전 포지션에만 집중하도록 허용되기 때문에, 입력(input) 순서와 함께, 어텐션 마스크(attention mask) 가 필요함\n",
        "- 언어 모델링 과제를 위해서, 다음의 포지션에 있는 모든 토큰들은 마스킹 되어야(가려져야) 함. 실제 단어를 얻기 위해서, nn.TransformerEncoder 의 출력은 로그-소프트맥스(log-Softmax) 로 이어지는 최종 선형(Linear) 레이어로 전달됨.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFtKtgzSIr1Z"
      },
      "source": [
        "모델정의\n",
        "----------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAU0CGwpIr1a"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn  # 파이토치의 신경망 패키지\n",
        "import torch.nn.functional as F # 활성화 함수, 손실 함수 등을 포함하는 모듈 (관례에 따라 일반적으로 F 네임스페이스로 임포트 됩니다) \n",
        "\n",
        "# nn.Module(자체가 클래스이고 상태를 추척할 수 있는) 파라미터로 사용할 수 있는 TransformerModel 클래스 구성\n",
        "\n",
        "# ntokens = len(TEXT.vocab.stoi) # 단어 사전의 크기\n",
        "# emsize(ninp) = 200 # 임베딩 차원\n",
        "# nhead = 2 # 멀티헤드 어텐션(multi-head attention) 모델의 헤드 개수\n",
        "# nhid = 200 # nn.TransformerEncoder 에서 피드포워드 네트워크(feedforward network) 모델의 차원\n",
        "# nlayers = 2 # nn.TransformerEncoder 내부의 nn.TransformerEncoderLayer 개수\n",
        "# dropout # 드랍아웃(dropout) 값\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "        self.model_type = 'Transformer'\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src):\n",
        "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "            device = src.device\n",
        "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "            self.src_mask = mask\n",
        "\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4wc2sHyIr1c"
      },
      "source": [
        "# PositionalEncoding 모듈\n",
        "- 시퀀스 안에서 토큰의 상대적인 또는 절대적인 포지션에 대한 정보를 생성 \n",
        "- 포지셔널 인코딩은 임베딩과 합칠 수 있도록 똑같은 차원으로 구성\n",
        "- 다른 주파수(frequency) 의 sine 과 cosine 함수를 사용함\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5GjDEWPIr1c"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model) # 스칼라 값으로 채워진 텐서를 반환\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfZAOZM1Ir1e"
      },
      "source": [
        "데이터 로드하고 배치 만들기\n",
        "-------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kj9xep0aIr1f"
      },
      "source": [
        "학습 과정에서는 torchtext 의 Wikitext-2 데이터셋을 이용\n",
        "\n",
        "단어 오브젝트는 훈련 데이터셋(train dataset) 에 의하여 만들어지고, 토큰을 텐서(tensor)로 수치화하는데 사용\n",
        "\n",
        "시퀀스 데이터로부터 시작하여, batchify() 함수는 데이터셋을 컬럼들로 배열하고, batch_size 사이즈의 배치들로 나눈 후에 남은 모든 토큰을 버림\n",
        "\n",
        "예를 들어, 알파벳을 시퀀스(총 길이 26) 로 생각하고 배치 사이즈를 4라고 한다면, 우리는 알파벳을 길이가 6인 4개의 시퀀스로 나눌 수 있음\n",
        "\n",
        "\\begin{align}\\begin{bmatrix}\n",
        "  \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n",
        "  \\end{bmatrix}\n",
        "  \\Rightarrow\n",
        "  \\begin{bmatrix}\n",
        "  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
        "  \\end{bmatrix}\\end{align}\n",
        "\n",
        "이 컬럼들은 모델에 의해서 독립적으로 취급되며, 이것은 더 효율적인 배치 프로세싱(batch processing) 이 가능하지만, G 와 F 의 의존성이 학습될 수 없다는 것을 의미함\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZNHf1C9IuMr",
        "outputId": "eab5589a-ba5e-4c8b-90a7-a67849fd09be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip3 install torchtext==0.4.0"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.4.0\n",
            "  Downloading torchtext-0.4.0-py3-none-any.whl (53 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████▏                         | 10 kB 23.7 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 20 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 30 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 40 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 51 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 53 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0) (4.64.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0) (2.23.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0) (1.11.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0) (1.21.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0) (2.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.4.0) (4.2.0)\n",
            "Installing collected packages: torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.12.0\n",
            "    Uninstalling torchtext-0.12.0:\n",
            "      Successfully uninstalled torchtext-0.12.0\n",
            "Successfully installed torchtext-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bteCqnOIr1f",
        "outputId": "7f2ca306-1c35-46fd-e069-a0f0d484847e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n",
        "                            init_token='<sos>',\n",
        "                            eos_token='<eos>',\n",
        "                            lower=True)\n",
        "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
        "TEXT.build_vocab(train_txt)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    data = TEXT.numericalize([data.examples[0].text])\n",
        "    # 데이터셋을 bsz 파트들로 나눕니다.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # 깔끔하게 나누어 떨어지지 않는 추가적인 부분(나머지들) 은 잘라냅니다.\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # 데이터에 대하여 bsz 배치들로 동등하게 나눕니다.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_txt, batch_size)\n",
        "val_data = batchify(val_txt, eval_batch_size)\n",
        "test_data = batchify(test_txt, eval_batch_size)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading wikitext-2-v1.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "wikitext-2-v1.zip: 100%|██████████| 4.48M/4.48M [00:00<00:00, 17.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "extracting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka726BR4Ir1h"
      },
      "source": [
        "# 입력(input) 과 타겟(target) 시퀀스를 생성하기 위한 함수들"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LRUmjbYIr1h"
      },
      "source": [
        "get_batch() 함수는 트랜스포머 모델을 위한 입력과 타겟 시퀀스를 생성합\n",
        "\n",
        "![](https://github.com/pytorch/tutorials/blob/gh-pages/_static/img/transformer_input_target.png?raw=1)\n",
        "\n",
        "\n",
        "변수는 트랜스포머 모델의 S 차원과 일치하는 0 차원에 해당합니다. 배치 차원 N 은 1 차원에 해당합니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvy4pkNmIr1i"
      },
      "source": [
        "bptt = 35\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Pgs7owaIr1j"
      },
      "source": [
        "인스턴스(instance) 초기화하기\n",
        "--------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUYRrMUUIr1k"
      },
      "source": [
        "모델은 아래와 같은 하이퍼파라미터(hyperparameter) 로 세팅 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajFK83wiIr1k"
      },
      "source": [
        "ntokens = len(TEXT.vocab.stoi) # 단어 사전의 크기\n",
        "emsize = 200 # 임베딩 차원\n",
        "nhid = 200 # nn.TransformerEncoder 에서 피드포워드 네트워크(feedforward network) 모델의 차원\n",
        "nlayers = 2 # nn.TransformerEncoder 내부의 nn.TransformerEncoderLayer 개수\n",
        "nhead = 2 # 멀티헤드 어텐션(multi-head attention) 모델의 헤드 개수\n",
        "dropout = 0.2 # 드랍아웃(dropout) 값\n",
        "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCZE2G5RIr1m"
      },
      "source": [
        "모델 실행하기\n",
        "-------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp26AfVkIr1n"
      },
      "source": [
        "손실(loss) 을 추적하는 데에는 CrossEntropyLoss 가 적용되며, 옵티마이저(optimizer) 로서 SGD 는 확률적 경사 하강법(stochastic gradient descent method) 을 구현합니다. 초기 학습률(learning rate) 은 5.0 으로 설정됩니다. StepLR 은 에포크(epoch) 에 따라서 학습률을 조절하는데 사용됩니다. 학습하는 동안에, 우리는 기울기 폭발(gradient exploding) 을 방지하기 위하여 모든 기울기를 함께 스케일(scale) 하는 함수인 nn.utils.clip_grad_norm_ 을 이용합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dp-1CUdIr1n"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0 # 학습률\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "import time\n",
        "def train():\n",
        "    model.train() # 학습 모드를 시작합니다.\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        log_interval = 200\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
        "                    elapsed * 1000 / log_interval,\n",
        "                    cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(eval_model, data_source):\n",
        "    eval_model.eval() # 평가 모드를 시작합니다.\n",
        "    total_loss = 0.\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output = eval_model(data)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIA8FNJ4Ir1p"
      },
      "source": [
        "에포크 내에서 반복됩니다. 만약 검증 오차(validation loss) 가 우리가 지금까지 관찰한 것 중 최적이라면 모델을 저장합니다. 매 에포크 이후에 학습률을 조절합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKjB6vXYIr1p",
        "outputId": "4c634f0c-70f5-4959-90c7-ba2535d057b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "best_val_loss = float(\"inf\")\n",
        "epochs = 10 # The number of epochs (테스트를 위해서 최소 수준으로 조정)\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train()\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                     val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = model\n",
        "\n",
        "    scheduler.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:372: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/ 2981 batches | lr 5.00 | ms/batch 38.74 | loss  8.05 | ppl  3137.49\n",
            "| epoch   1 |   400/ 2981 batches | lr 5.00 | ms/batch 37.02 | loss  6.78 | ppl   876.03\n",
            "| epoch   1 |   600/ 2981 batches | lr 5.00 | ms/batch 35.86 | loss  6.36 | ppl   576.50\n",
            "| epoch   1 |   800/ 2981 batches | lr 5.00 | ms/batch 36.06 | loss  6.23 | ppl   506.14\n",
            "| epoch   1 |  1000/ 2981 batches | lr 5.00 | ms/batch 36.12 | loss  6.11 | ppl   451.33\n",
            "| epoch   1 |  1200/ 2981 batches | lr 5.00 | ms/batch 39.85 | loss  6.08 | ppl   438.19\n",
            "| epoch   1 |  1400/ 2981 batches | lr 5.00 | ms/batch 35.98 | loss  6.04 | ppl   420.44\n",
            "| epoch   1 |  1600/ 2981 batches | lr 5.00 | ms/batch 36.10 | loss  6.05 | ppl   422.99\n",
            "| epoch   1 |  1800/ 2981 batches | lr 5.00 | ms/batch 36.17 | loss  5.95 | ppl   383.86\n",
            "| epoch   1 |  2000/ 2981 batches | lr 5.00 | ms/batch 36.76 | loss  5.96 | ppl   385.90\n",
            "| epoch   1 |  2200/ 2981 batches | lr 5.00 | ms/batch 36.00 | loss  5.85 | ppl   346.27\n",
            "| epoch   1 |  2400/ 2981 batches | lr 5.00 | ms/batch 36.09 | loss  5.89 | ppl   361.97\n",
            "| epoch   1 |  2600/ 2981 batches | lr 5.00 | ms/batch 36.03 | loss  5.89 | ppl   360.18\n",
            "| epoch   1 |  2800/ 2981 batches | lr 5.00 | ms/batch 35.95 | loss  5.79 | ppl   328.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 113.02s | valid loss  5.68 | valid ppl   293.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 2981 batches | lr 4.51 | ms/batch 36.26 | loss  5.80 | ppl   329.48\n",
            "| epoch   2 |   400/ 2981 batches | lr 4.51 | ms/batch 35.97 | loss  5.77 | ppl   320.59\n",
            "| epoch   2 |   600/ 2981 batches | lr 4.51 | ms/batch 36.03 | loss  5.60 | ppl   269.75\n",
            "| epoch   2 |   800/ 2981 batches | lr 4.51 | ms/batch 36.00 | loss  5.63 | ppl   279.31\n",
            "| epoch   2 |  1000/ 2981 batches | lr 4.51 | ms/batch 36.02 | loss  5.59 | ppl   267.09\n",
            "| epoch   2 |  1200/ 2981 batches | lr 4.51 | ms/batch 36.01 | loss  5.61 | ppl   273.75\n",
            "| epoch   2 |  1400/ 2981 batches | lr 4.51 | ms/batch 35.92 | loss  5.62 | ppl   276.87\n",
            "| epoch   2 |  1600/ 2981 batches | lr 4.51 | ms/batch 35.93 | loss  5.66 | ppl   286.56\n",
            "| epoch   2 |  1800/ 2981 batches | lr 4.51 | ms/batch 35.98 | loss  5.58 | ppl   265.55\n",
            "| epoch   2 |  2000/ 2981 batches | lr 4.51 | ms/batch 36.01 | loss  5.61 | ppl   272.71\n",
            "| epoch   2 |  2200/ 2981 batches | lr 4.51 | ms/batch 35.89 | loss  5.51 | ppl   246.59\n",
            "| epoch   2 |  2400/ 2981 batches | lr 4.51 | ms/batch 35.91 | loss  5.57 | ppl   262.80\n",
            "| epoch   2 |  2600/ 2981 batches | lr 4.51 | ms/batch 35.92 | loss  5.58 | ppl   265.69\n",
            "| epoch   2 |  2800/ 2981 batches | lr 4.51 | ms/batch 35.97 | loss  5.50 | ppl   245.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 111.09s | valid loss  5.65 | valid ppl   285.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 2981 batches | lr 4.29 | ms/batch 36.22 | loss  5.55 | ppl   257.71\n",
            "| epoch   3 |   400/ 2981 batches | lr 4.29 | ms/batch 35.99 | loss  5.56 | ppl   258.82\n",
            "| epoch   3 |   600/ 2981 batches | lr 4.29 | ms/batch 36.12 | loss  5.36 | ppl   213.08\n",
            "| epoch   3 |   800/ 2981 batches | lr 4.29 | ms/batch 36.07 | loss  5.42 | ppl   226.28\n",
            "| epoch   3 |  1000/ 2981 batches | lr 4.29 | ms/batch 36.05 | loss  5.38 | ppl   217.57\n",
            "| epoch   3 |  1200/ 2981 batches | lr 4.29 | ms/batch 36.00 | loss  5.41 | ppl   222.59\n",
            "| epoch   3 |  1400/ 2981 batches | lr 4.29 | ms/batch 36.10 | loss  5.44 | ppl   229.35\n",
            "| epoch   3 |  1600/ 2981 batches | lr 4.29 | ms/batch 36.10 | loss  5.48 | ppl   240.01\n",
            "| epoch   3 |  1800/ 2981 batches | lr 4.29 | ms/batch 36.12 | loss  5.41 | ppl   222.79\n",
            "| epoch   3 |  2000/ 2981 batches | lr 4.29 | ms/batch 35.92 | loss  5.44 | ppl   230.75\n",
            "| epoch   3 |  2200/ 2981 batches | lr 4.29 | ms/batch 35.97 | loss  5.33 | ppl   205.61\n",
            "| epoch   3 |  2400/ 2981 batches | lr 4.29 | ms/batch 36.51 | loss  5.40 | ppl   220.51\n",
            "| epoch   3 |  2600/ 2981 batches | lr 4.29 | ms/batch 36.20 | loss  5.42 | ppl   225.20\n",
            "| epoch   3 |  2800/ 2981 batches | lr 4.29 | ms/batch 35.93 | loss  5.36 | ppl   211.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 111.35s | valid loss  5.50 | valid ppl   243.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/ 2981 batches | lr 4.07 | ms/batch 36.30 | loss  5.39 | ppl   218.81\n",
            "| epoch   4 |   400/ 2981 batches | lr 4.07 | ms/batch 36.01 | loss  5.40 | ppl   220.75\n",
            "| epoch   4 |   600/ 2981 batches | lr 4.07 | ms/batch 36.08 | loss  5.20 | ppl   181.18\n",
            "| epoch   4 |   800/ 2981 batches | lr 4.07 | ms/batch 35.92 | loss  5.27 | ppl   194.50\n",
            "| epoch   4 |  1000/ 2981 batches | lr 4.07 | ms/batch 36.14 | loss  5.23 | ppl   185.97\n",
            "| epoch   4 |  1200/ 2981 batches | lr 4.07 | ms/batch 36.12 | loss  5.26 | ppl   191.98\n",
            "| epoch   4 |  1400/ 2981 batches | lr 4.07 | ms/batch 36.03 | loss  5.28 | ppl   196.62\n",
            "| epoch   4 |  1600/ 2981 batches | lr 4.07 | ms/batch 36.06 | loss  5.34 | ppl   208.43\n",
            "| epoch   4 |  1800/ 2981 batches | lr 4.07 | ms/batch 36.04 | loss  5.26 | ppl   193.13\n",
            "| epoch   4 |  2000/ 2981 batches | lr 4.07 | ms/batch 35.97 | loss  5.29 | ppl   199.06\n",
            "| epoch   4 |  2200/ 2981 batches | lr 4.07 | ms/batch 36.01 | loss  5.18 | ppl   177.70\n",
            "| epoch   4 |  2400/ 2981 batches | lr 4.07 | ms/batch 35.91 | loss  5.26 | ppl   192.27\n",
            "| epoch   4 |  2600/ 2981 batches | lr 4.07 | ms/batch 35.91 | loss  5.28 | ppl   196.24\n",
            "| epoch   4 |  2800/ 2981 batches | lr 4.07 | ms/batch 35.88 | loss  5.21 | ppl   182.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 111.19s | valid loss  5.46 | valid ppl   235.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/ 2981 batches | lr 3.87 | ms/batch 36.10 | loss  5.26 | ppl   193.17\n",
            "| epoch   5 |   400/ 2981 batches | lr 3.87 | ms/batch 35.85 | loss  5.28 | ppl   195.41\n",
            "| epoch   5 |   600/ 2981 batches | lr 3.87 | ms/batch 35.91 | loss  5.08 | ppl   161.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d4mtUeWIr1r"
      },
      "source": [
        "평가 데이터셋(test dataset) 으로 모델을 평가하기\n",
        "-------------------------------------\n",
        "\n",
        "평가 데이터셋에 대한 결과를 확인하기 위해서 최고의 모델을 적용합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XscTuywEIr1r"
      },
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}